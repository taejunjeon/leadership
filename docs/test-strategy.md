# Grid 3.0 ë¦¬ë”ì‹­ ë§¤í•‘ í”Œë«í¼ í…ŒìŠ¤íŠ¸ ì „ëµ

> ìµœì¢… ì—…ë°ì´íŠ¸: 2025-08-02  
> ì‘ì„±ì: í—¤íŒŒì´ìŠ¤í† ìŠ¤  
> í”„ë¡œì íŠ¸: Grid 3.0 Leadership Mapping Platform  
> í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬: Jest, RTL, Pytest, Playwright

## ëª©ì°¨

1. [í…ŒìŠ¤íŠ¸ ì „ëµ ê°œìš”](#1-í…ŒìŠ¤íŠ¸-ì „ëµ-ê°œìš”)
2. [í…ŒìŠ¤íŠ¸ í”¼ë¼ë¯¸ë“œ](#2-í…ŒìŠ¤íŠ¸-í”¼ë¼ë¯¸ë“œ)
3. [ë‹¨ìœ„ í…ŒìŠ¤íŠ¸](#3-ë‹¨ìœ„-í…ŒìŠ¤íŠ¸)
4. [í†µí•© í…ŒìŠ¤íŠ¸](#4-í†µí•©-í…ŒìŠ¤íŠ¸)
5. [E2E í…ŒìŠ¤íŠ¸](#5-e2e-í…ŒìŠ¤íŠ¸)
6. [ì„±ëŠ¥ í…ŒìŠ¤íŠ¸](#6-ì„±ëŠ¥-í…ŒìŠ¤íŠ¸)
7. [í…ŒìŠ¤íŠ¸ ë°ì´í„° ê´€ë¦¬](#7-í…ŒìŠ¤íŠ¸-ë°ì´í„°-ê´€ë¦¬)
8. [CI/CD í…ŒìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸](#8-cicd-í…ŒìŠ¤íŠ¸-íŒŒì´í”„ë¼ì¸)
9. [í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­](#9-í…ŒìŠ¤íŠ¸-ë©”íŠ¸ë¦­)
10. [í…ŒìŠ¤íŠ¸ ë„êµ¬ ë° ì„¤ì •](#10-í…ŒìŠ¤íŠ¸-ë„êµ¬-ë°-ì„¤ì •)

## 1. í…ŒìŠ¤íŠ¸ ì „ëµ ê°œìš”

### 1.1 í…ŒìŠ¤íŠ¸ ëª©í‘œ

Grid 3.0 í”Œë«í¼ì˜ í…ŒìŠ¤íŠ¸ ì „ëµì€ ë‹¤ìŒ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤:

- **ê¸°ëŠ¥ ì •í™•ì„±**: 4D ë¦¬ë”ì‹­ ì ìˆ˜ ê³„ì‚°ì˜ ì •í™•ì„± ë³´ì¥
- **ë°ì´í„° ë¬´ê²°ì„±**: ì„¤ë¬¸ ì‘ë‹µê³¼ ì½”ì¹­ ë°ì´í„°ì˜ ì¼ê´€ì„± ìœ ì§€
- **ì‚¬ìš©ì ê²½í—˜**: 3D ì‹œê°í™”ì™€ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ì˜ ì•ˆì •ì„±
- **ë³´ì•ˆ**: ì¸ì¦/ê¶Œí•œ ì‹œìŠ¤í…œì˜ ê²¬ê³ ì„±
- **ì„±ëŠ¥**: 200ëª… ë™ì‹œ ì ‘ì†ê³¼ 30ì´ˆ ì´ë‚´ ì‘ë‹µ ë³´ì¥

### 1.2 í…ŒìŠ¤íŠ¸ ì›ì¹™

```yaml
í•µì‹¬ ì›ì¹™:
  ì‹ ì†ì„±: "ë¹ ë¥¸ í”¼ë“œë°±ìœ¼ë¡œ ê°œë°œ ì†ë„ í–¥ìƒ"
  ì‹ ë¢°ì„±: "í…ŒìŠ¤íŠ¸ ê²°ê³¼ì— ëŒ€í•œ ì™„ì „í•œ ì‹ ë¢°"
  ìœ ì§€ë³´ìˆ˜ì„±: "í…ŒìŠ¤íŠ¸ ì½”ë“œë„ í”„ë¡œë•ì…˜ ì½”ë“œë§Œí¼ ì¤‘ìš”"
  ì‹¤ìš©ì„±: "ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ë¥¼ ê²€ì¦í•˜ëŠ” í…ŒìŠ¤íŠ¸ ìš°ì„ "
```

## 2. í…ŒìŠ¤íŠ¸ í”¼ë¼ë¯¸ë“œ

```
      ğŸ”º E2E (10%)
      ì‚¬ìš©ì ì‹œë‚˜ë¦¬ì˜¤ ê²€ì¦
      Playwrightë¡œ êµ¬í˜„
      
    ğŸ”ºğŸ”º í†µí•© (20%)
    API + DB ì—°ë™ í…ŒìŠ¤íŠ¸
    TestClient + TestDB
    
  ğŸ”ºğŸ”ºğŸ”º ë‹¨ìœ„ (70%)
  ê°œë³„ í•¨ìˆ˜/ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸
  Jest + RTL + Pytest
```

### 2.1 í…ŒìŠ¤íŠ¸ ë¹„ìœ¨ ëª©í‘œ

| í…ŒìŠ¤íŠ¸ ìœ í˜• | ë¹„ìœ¨ | ì‹¤í–‰ ì‹œê°„ | ëª©ì  |
|------------|------|-----------|------|
| ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ | 70% | < 10ì´ˆ | ë¡œì§ ê²€ì¦ |
| í†µí•© í…ŒìŠ¤íŠ¸ | 20% | < 30ì´ˆ | ì—°ë™ ê²€ì¦ |
| E2E í…ŒìŠ¤íŠ¸ | 10% | < 2ë¶„ | ì‹œë‚˜ë¦¬ì˜¤ ê²€ì¦ |

### 2.2 í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì „ëµ

```mermaid
graph LR
    A[ì½”ë“œ ë³€ê²½] --> B[ë‹¨ìœ„ í…ŒìŠ¤íŠ¸]
    B --> C{í†µê³¼?}
    C -->|Yes| D[í†µí•© í…ŒìŠ¤íŠ¸]
    C -->|No| A
    D --> E{í†µê³¼?}
    E -->|Yes| F[E2E í…ŒìŠ¤íŠ¸]
    E -->|No| A
    F --> G{í†µê³¼?}
    G -->|Yes| H[ë°°í¬]
    G -->|No| A
```

## 3. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

### 3.1 í”„ë¡ íŠ¸ì—”ë“œ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

#### í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ í…ŒìŠ¤íŠ¸

```typescript
// src/lib/__tests__/leadership-calculator.test.ts
import { describe, it, expect } from '@jest/globals';
import { calculateLeadershipScores, determineLeadershipStyle } from '../leadership-calculator';

describe('LeadershipCalculator', () => {
  describe('calculateLeadershipScores', () => {
    it('should calculate correct 4D scores for team-style leader', () => {
      // Given: íŒ€í˜• ë¦¬ë”ì˜ ì‘ë‹µ íŒ¨í„´
      const teamStyleAnswers = [
        7, 8, 7, 8, 7, 8, 7, 8,  // People Concern (í‰ê·  7.5)
        7, 8, 7, 8, 7, 8, 7, 8,  // Production Concern (í‰ê·  7.5)
        7, 7, 7, 7,              // Radical Candor - Care (í‰ê·  7.0)
        6, 6, 6, 6,              // Radical Candor - Challenge (í‰ê·  6.0)
        7, 7, 7, 7, 7, 7, 7      // LMX (í‰ê·  7.0)
      ];
      
      // When
      const scores = calculateLeadershipScores(teamStyleAnswers);
      
      // Then
      expect(scores.peopleConcern).toBe(7.5);
      expect(scores.productionConcern).toBe(7.5);
      expect(scores.candorLevel).toBe(6.5);
      expect(scores.lmxQuality).toBe(7.0);
    });
    
    it('should handle edge cases correctly', () => {
      // Given: ìµœì†Ÿê°’ ì‘ë‹µ
      const minAnswers = Array(31).fill(1);
      
      // When
      const scores = calculateLeadershipScores(minAnswers);
      
      // Then
      expect(scores.peopleConcern).toBe(1.0);
      expect(scores.productionConcern).toBe(1.0);
      expect(scores.candorLevel).toBe(1.0);
      expect(scores.lmxQuality).toBe(1.0);
    });
    
    it('should throw error for invalid input', () => {
      // Given: ì˜ëª»ëœ ê°œìˆ˜ì˜ ì‘ë‹µ
      const invalidAnswers = [1, 2, 3];
      
      // When & Then
      expect(() => calculateLeadershipScores(invalidAnswers))
        .toThrow('ì„¤ë¬¸ ì‘ë‹µì€ ì •í™•íˆ 31ê°œì—¬ì•¼ í•©ë‹ˆë‹¤');
    });
  });
  
  describe('determineLeadershipStyle', () => {
    const testCases = [
      { people: 8.0, production: 8.0, expected: 'íŒ€í˜•(8.0, 8.0)' },
      { people: 2.0, production: 8.0, expected: 'ê³¼ì—…í˜•(2.0, 8.0)' },
      { people: 8.0, production: 2.0, expected: 'ì»¨íŠ¸ë¦¬í´ëŸ½í˜•(8.0, 2.0)' },
      { people: 2.0, production: 2.0, expected: 'ë¬´ê´€ì‹¬í˜•(2.0, 2.0)' },
      { people: 5.5, production: 5.5, expected: 'ì¤‘ë„í˜•(5.5, 5.5)' }
    ];
    
    testCases.forEach(({ people, production, expected }) => {
      it(`should identify ${expected} correctly`, () => {
        const style = determineLeadershipStyle(people, production);
        expect(style).toBe(expected);
      });
    });
  });
});
```

#### React ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸

```typescript
// src/components/__tests__/LeadershipGrid3D.test.tsx
import { render, screen, fireEvent, waitFor } from '@testing-library/react';
import { Canvas } from '@react-three/fiber';
import { LeadershipGrid3D } from '../LeadershipGrid3D';

// Mock Three.js components
jest.mock('@react-three/fiber', () => ({
  Canvas: ({ children }: { children: React.ReactNode }) => <div data-testid="canvas">{children}</div>,
  useFrame: jest.fn(),
  useThree: () => ({ camera: {}, gl: {} })
}));

describe('LeadershipGrid3D', () => {
  const mockLeaders = [
    {
      id: '1',
      name: 'ê¹€íŒ€í˜•',
      scores: { peopleConcern: 8.0, productionConcern: 8.0, candorLevel: 7.0, lmxQuality: 7.5 }
    },
    {
      id: '2', 
      name: 'ë°•ê³¼ì—…',
      scores: { peopleConcern: 3.0, productionConcern: 8.5, candorLevel: 5.0, lmxQuality: 6.0 }
    }
  ];
  
  it('should render 3D canvas with leader points', async () => {
    render(<LeadershipGrid3D leaders={mockLeaders} />);
    
    expect(screen.getByTestId('canvas')).toBeInTheDocument();
  });
  
  it('should call onLeaderClick when leader point is clicked', async () => {
    const mockOnClick = jest.fn();
    
    render(
      <LeadershipGrid3D 
        leaders={mockLeaders} 
        onLeaderClick={mockOnClick} 
      />
    );
    
    // 3D ì¸í„°ë™ì…˜ì€ ëª¨í‚¹ëœ ìƒíƒœì—ì„œ ì‹œë®¬ë ˆì´ì…˜
    const canvas = screen.getByTestId('canvas');
    fireEvent.click(canvas);
    
    await waitFor(() => {
      expect(mockOnClick).toHaveBeenCalled();
    });
  });
  
  it('should update positions when leader data changes', () => {
    const { rerender } = render(<LeadershipGrid3D leaders={mockLeaders} />);
    
    const updatedLeaders = [...mockLeaders];
    updatedLeaders[0].scores.peopleConcern = 9.0;
    
    rerender(<LeadershipGrid3D leaders={updatedLeaders} />);
    
    // ìœ„ì¹˜ ì—…ë°ì´íŠ¸ ê²€ì¦ì€ í…ŒìŠ¤íŠ¸ìš© í—¬í¼ í•¨ìˆ˜ë¡œ ì²˜ë¦¬
    expect(screen.getByTestId('canvas')).toBeInTheDocument();
  });
});
```

#### ì»¤ìŠ¤í…€ í›… í…ŒìŠ¤íŠ¸

```typescript
// src/hooks/__tests__/useLeadershipData.test.ts
import { renderHook, waitFor } from '@testing-library/react';
import { QueryClient, QueryClientProvider } from '@tanstack/react-query';
import { useLeadershipData } from '../useLeadershipData';
import { apiClient } from '../api-client';

// API ëª¨í‚¹
jest.mock('../api-client');
const mockApiClient = apiClient as jest.Mocked<typeof apiClient>;

describe('useLeadershipData', () => {
  let queryClient: QueryClient;
  
  beforeEach(() => {
    queryClient = new QueryClient({
      defaultOptions: { queries: { retry: false } }
    });
  });
  
  const wrapper = ({ children }: { children: React.ReactNode }) => (
    <QueryClientProvider client={queryClient}>{children}</QueryClientProvider>
  );
  
  it('should fetch and return leadership data', async () => {
    // Given
    const mockData = {
      leaders: [{ id: '1', name: 'ê¹€ë¦¬ë”', scores: { /* ... */ } }]
    };
    mockApiClient.getLeaders.mockResolvedValue(mockData);
    
    // When
    const { result } = renderHook(() => useLeadershipData(), { wrapper });
    
    // Then
    await waitFor(() => {
      expect(result.current.isSuccess).toBe(true);
    });
    
    expect(result.current.data).toEqual(mockData);
    expect(mockApiClient.getLeaders).toHaveBeenCalledTimes(1);
  });
  
  it('should handle error states', async () => {
    // Given
    const error = new Error('Network error');
    mockApiClient.getLeaders.mockRejectedValue(error);
    
    // When
    const { result } = renderHook(() => useLeadershipData(), { wrapper });
    
    // Then
    await waitFor(() => {
      expect(result.current.isError).toBe(true);
    });
    
    expect(result.current.error).toEqual(error);
  });
});
```

### 3.2 ë°±ì—”ë“œ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

#### ê³„ì‚° ë¡œì§ í…ŒìŠ¤íŠ¸

```python
# tests/unit/test_calculations.py
import pytest
from app.services.calculation import calculate_4d_scores, determine_leadership_style

class TestCalculations:
    def test_calculate_4d_scores_team_style(self):
        """íŒ€í˜• ë¦¬ë”ì˜ 4D ì ìˆ˜ ê³„ì‚° í…ŒìŠ¤íŠ¸"""
        # Given: íŒ€í˜• íŒ¨í„´ì˜ ì‘ë‹µ (ë†’ì€ ì‚¬ëŒ/ì„±ê³¼ ê´€ì‹¬)
        answers = [7, 8, 7, 8, 7, 8, 7, 8] * 2 + [7, 7, 6, 6] + [7] * 7
        
        # When
        scores = calculate_4d_scores(answers)
        
        # Then
        assert scores.people_concern == 7.5
        assert scores.production_concern == 7.5
        assert scores.candor_level == 6.5
        assert scores.lmx_quality == 7.0
    
    def test_calculate_4d_scores_edge_cases(self):
        """ê²½ê³„ê°’ í…ŒìŠ¤íŠ¸"""
        # Given: ëª¨ë“  ì‘ë‹µì´ ìµœì†Ÿê°’
        min_answers = [1] * 31
        
        # When
        scores = calculate_4d_scores(min_answers)
        
        # Then
        assert scores.people_concern == 1.0
        assert scores.production_concern == 1.0
        assert scores.candor_level == 1.0
        assert scores.lmx_quality == 1.0
        
        # Given: ëª¨ë“  ì‘ë‹µì´ ìµœëŒ“ê°’
        max_answers = [7] * 31
        
        # When
        scores = calculate_4d_scores(max_answers)
        
        # Then
        assert scores.people_concern == 7.0
        assert scores.production_concern == 7.0
        assert scores.candor_level == 7.0
        assert scores.lmx_quality == 7.0
    
    def test_calculate_4d_scores_invalid_input(self):
        """ì˜ëª»ëœ ì…ë ¥ê°’ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        with pytest.raises(ValueError, match="31ê°œì˜ ì‘ë‹µì´ í•„ìš”í•©ë‹ˆë‹¤"):
            calculate_4d_scores([1, 2, 3])
        
        with pytest.raises(ValueError, match="1-7 ë²”ìœ„ì˜ ê°’ì´ì–´ì•¼ í•©ë‹ˆë‹¤"):
            calculate_4d_scores([0] + [5] * 30)
    
    @pytest.mark.parametrize("people,production,expected", [
        (8.0, 8.0, "íŒ€í˜•(8.0, 8.0)"),
        (2.0, 8.0, "ê³¼ì—…í˜•(2.0, 8.0)"),
        (8.0, 2.0, "ì»¨íŠ¸ë¦¬í´ëŸ½í˜•(8.0, 2.0)"),
        (2.0, 2.0, "ë¬´ê´€ì‹¬í˜•(2.0, 2.0)"),
        (5.5, 5.5, "ì¤‘ë„í˜•(5.5, 5.5)")
    ])
    def test_determine_leadership_style(self, people, production, expected):
        """ë¦¬ë”ì‹­ ìŠ¤íƒ€ì¼ ë¶„ë¥˜ í…ŒìŠ¤íŠ¸"""
        style = determine_leadership_style(people, production)
        assert style == expected
```

#### Pydantic ëª¨ë¸ í…ŒìŠ¤íŠ¸

```python
# tests/unit/test_schemas.py
import pytest
from pydantic import ValidationError
from app.schemas.leader import LeaderCreate, SurveyResponseCreate

class TestSchemas:
    def test_leader_create_valid(self):
        """ìœ íš¨í•œ ë¦¬ë” ìƒì„± ë°ì´í„° í…ŒìŠ¤íŠ¸"""
        data = {
            "name": "ê¹€ë¦¬ë”",
            "email": "leader@test.com",
            "team": "engineering"
        }
        
        leader = LeaderCreate(**data)
        
        assert leader.name == "ê¹€ë¦¬ë”"
        assert leader.email == "leader@test.com"
        assert leader.team == "engineering"
    
    def test_leader_create_invalid_email(self):
        """ì˜ëª»ëœ ì´ë©”ì¼ í˜•ì‹ í…ŒìŠ¤íŠ¸"""
        data = {
            "name": "ê¹€ë¦¬ë”",
            "email": "invalid-email",
            "team": "engineering"
        }
        
        with pytest.raises(ValidationError) as exc_info:
            LeaderCreate(**data)
        
        assert "email" in str(exc_info.value)
    
    def test_survey_response_create_valid(self):
        """ìœ íš¨í•œ ì„¤ë¬¸ ì‘ë‹µ ë°ì´í„° í…ŒìŠ¤íŠ¸"""
        data = {
            "leader_id": "550e8400-e29b-41d4-a716-446655440000",
            "answers": [5] * 31
        }
        
        response = SurveyResponseCreate(**data)
        
        assert len(response.answers) == 31
        assert all(1 <= answer <= 7 for answer in response.answers)
    
    def test_survey_response_invalid_answer_count(self):
        """ì˜ëª»ëœ ì‘ë‹µ ê°œìˆ˜ í…ŒìŠ¤íŠ¸"""
        data = {
            "leader_id": "550e8400-e29b-41d4-a716-446655440000",
            "answers": [5] * 30  # 31ê°œê°€ ì•„ë‹˜
        }
        
        with pytest.raises(ValidationError):
            SurveyResponseCreate(**data)
    
    def test_survey_response_invalid_answer_range(self):
        """ì‘ë‹µ ë²”ìœ„ ë²—ì–´ë‚¨ í…ŒìŠ¤íŠ¸"""
        data = {
            "leader_id": "550e8400-e29b-41d4-a716-446655440000",
            "answers": [0] + [5] * 30  # 0ì€ ìœ íš¨í•˜ì§€ ì•ŠìŒ
        }
        
        with pytest.raises(ValidationError):
            SurveyResponseCreate(**data)
```

## 4. í†µí•© í…ŒìŠ¤íŠ¸

### 4.1 API í†µí•© í…ŒìŠ¤íŠ¸

```python
# tests/integration/test_api_leaders.py
import pytest
from httpx import AsyncClient
from app.main import app
from app.core.database import get_db
from tests.conftest import override_get_db

class TestLeadersAPI:
    @pytest.mark.asyncio
    async def test_create_and_get_leader(self):
        """ë¦¬ë” ìƒì„± í›„ ì¡°íšŒ í†µí•© í…ŒìŠ¤íŠ¸"""
        app.dependency_overrides[get_db] = override_get_db
        
        async with AsyncClient(app=app, base_url="http://test") as ac:
            # Given: ìƒˆ ë¦¬ë” ë°ì´í„°
            leader_data = {
                "name": "ê¹€ì‹ ì…",
                "email": "newbie@test.com",
                "team": "engineering"
            }
            
            # When: ë¦¬ë” ìƒì„±
            create_response = await ac.post("/api/v1/leaders", json=leader_data)
            
            # Then: ìƒì„± ì„±ê³µ
            assert create_response.status_code == 201
            created_leader = create_response.json()
            leader_id = created_leader["id"]
            
            # When: ìƒì„±ëœ ë¦¬ë” ì¡°íšŒ
            get_response = await ac.get(f"/api/v1/leaders/{leader_id}")
            
            # Then: ì¡°íšŒ ì„±ê³µ ë° ë°ì´í„° ì¼ì¹˜
            assert get_response.status_code == 200
            retrieved_leader = get_response.json()
            assert retrieved_leader["name"] == leader_data["name"]
            assert retrieved_leader["email"] == leader_data["email"]
    
    @pytest.mark.asyncio
    async def test_survey_submission_and_score_calculation(self):
        """ì„¤ë¬¸ ì œì¶œ í›„ ì ìˆ˜ ê³„ì‚° í†µí•© í…ŒìŠ¤íŠ¸"""
        app.dependency_overrides[get_db] = override_get_db
        
        async with AsyncClient(app=app, base_url="http://test") as ac:
            # Given: ë¦¬ë” ìƒì„±
            leader_response = await ac.post("/api/v1/leaders", json={
                "name": "í…ŒìŠ¤íŠ¸ë¦¬ë”",
                "email": "test@example.com",
                "team": "test"
            })
            leader_id = leader_response.json()["id"]
            
            # Given: ì„¤ë¬¸ ì‘ë‹µ ë°ì´í„° (íŒ€í˜• íŒ¨í„´)
            survey_data = {
                "leader_id": leader_id,
                "answers": [7, 8, 7, 8, 7, 8, 7, 8] * 2 + [7, 7, 6, 6] + [7] * 7
            }
            
            # When: ì„¤ë¬¸ ì œì¶œ
            survey_response = await ac.post("/api/v1/survey-responses", json=survey_data)
            
            # Then: ì œì¶œ ì„±ê³µ ë° ì ìˆ˜ ê³„ì‚° í™•ì¸
            assert survey_response.status_code == 201
            response_data = survey_response.json()
            
            assert response_data["people_concern"] == 7.5
            assert response_data["production_concern"] == 7.5
            assert response_data["leadership_style"] == "íŒ€í˜•(7.5, 7.5)"
            
            # When: ë¦¬ë” ë°ì´í„° ë‹¤ì‹œ ì¡°íšŒ
            updated_leader = await ac.get(f"/api/v1/leaders/{leader_id}")
            
            # Then: ìµœì‹  ì ìˆ˜ê°€ ë°˜ì˜ë¨
            leader_data = updated_leader.json()
            assert leader_data["current_scores"]["people_concern"] == 7.5
```

### 4.2 ë°ì´í„°ë² ì´ìŠ¤ í†µí•© í…ŒìŠ¤íŠ¸

```python
# tests/integration/test_database_operations.py
import pytest
from sqlalchemy.ext.asyncio import AsyncSession
from app.models.leader import Leader
from app.models.survey_response import SurveyResponse
from app.repositories.leader_repository import LeaderRepository

class TestDatabaseOperations:
    @pytest.mark.asyncio
    async def test_leader_creation_with_transaction(self, db_session: AsyncSession):
        """íŠ¸ëœì­ì…˜ ë‚´ì—ì„œ ë¦¬ë” ìƒì„± í…ŒìŠ¤íŠ¸"""
        repo = LeaderRepository(db_session)
        
        # Given
        leader_data = {
            "name": "íŠ¸ëœì­ì…˜í…ŒìŠ¤íŠ¸",
            "email": "transaction@test.com",
            "team": "test"
        }
        
        # When
        async with db_session.begin():
            leader = await repo.create(leader_data)
            assert leader.id is not None
            
            # íŠ¸ëœì­ì…˜ ë‚´ì—ì„œ ì¡°íšŒ ê°€ëŠ¥
            found_leader = await repo.get_by_id(leader.id)
            assert found_leader is not None
            assert found_leader.name == leader_data["name"]
        
        # Then: íŠ¸ëœì­ì…˜ ì»¤ë°‹ í›„ì—ë„ ë°ì´í„° ì¡´ì¬
        committed_leader = await repo.get_by_id(leader.id)
        assert committed_leader is not None
    
    @pytest.mark.asyncio
    async def test_survey_response_triggers(self, db_session: AsyncSession):
        """ì„¤ë¬¸ ì‘ë‹µ ì €ì¥ ì‹œ íŠ¸ë¦¬ê±° ë™ì‘ í…ŒìŠ¤íŠ¸"""
        # Given: ë¦¬ë” ìƒì„±
        leader = Leader(
            name="íŠ¸ë¦¬ê±°í…ŒìŠ¤íŠ¸",
            email="trigger@test.com",
            team="test"
        )
        db_session.add(leader)
        await db_session.commit()
        await db_session.refresh(leader)
        
        # Given: ì„¤ë¬¸ ì‘ë‹µ ë°ì´í„°
        answers = [6] * 31
        
        # When: ì„¤ë¬¸ ì‘ë‹µ ì €ì¥
        survey_response = SurveyResponse(
            leader_id=leader.id,
            answers=answers
        )
        db_session.add(survey_response)
        await db_session.commit()
        await db_session.refresh(survey_response)
        
        # Then: íŠ¸ë¦¬ê±°ì— ì˜í•´ ì ìˆ˜ê°€ ìë™ ê³„ì‚°ë¨
        assert survey_response.people_concern == 6.0
        assert survey_response.production_concern == 6.0
        assert survey_response.candor_level == 6.0
        assert survey_response.lmx_quality == 6.0
        assert survey_response.leadership_style == "ì¤‘ë„í˜•(6.0, 6.0)"
```

## 5. E2E í…ŒìŠ¤íŠ¸

### 5.1 Playwright E2E í…ŒìŠ¤íŠ¸

```typescript
// tests/e2e/leadership-dashboard.spec.ts
import { test, expect, Page } from '@playwright/test';

test.describe('Leadership Dashboard E2E', () => {
  test.beforeEach(async ({ page }) => {
    // í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¤ì •
    await page.goto('/');
    await page.waitForLoadState('networkidle');
  });
  
  test('should display 3D leadership grid with leader points', async ({ page }) => {
    // Given: ëŒ€ì‹œë³´ë“œ í˜ì´ì§€ ì ‘ì†
    await page.goto('/dashboard');
    
    // When: 3D ê·¸ë¦¬ë“œ ë¡œë”© ëŒ€ê¸°
    await page.waitForSelector('[data-testid="leadership-grid-3d"]');
    
    // Then: ë¦¬ë” í¬ì¸íŠ¸ë“¤ì´ í‘œì‹œë¨
    const leaderPoints = page.locator('[data-testid="leader-point"]');
    await expect(leaderPoints).toHaveCount.greaterThan(0);
    
    // Then: ì¶• ë¼ë²¨ì´ ì˜¬ë°”ë¥´ê²Œ í‘œì‹œë¨
    await expect(page.locator('text=ì‚¬ëŒ ê´€ì‹¬')).toBeVisible();
    await expect(page.locator('text=ì„±ê³¼ ê´€ì‹¬')).toBeVisible();
    await expect(page.locator('text=ì§ë©´ ìˆ˜ì¤€')).toBeVisible();
  });
  
  test('should show coaching cards when leader point is clicked', async ({ page }) => {
    await page.goto('/dashboard');
    
    // Given: ì²« ë²ˆì§¸ ë¦¬ë” í¬ì¸íŠ¸ í´ë¦­
    const firstLeaderPoint = page.locator('[data-testid="leader-point"]').first();
    await firstLeaderPoint.click();
    
    // When: ì½”ì¹­ ì¹´ë“œ íŒ¨ë„ì´ ì—´ë¦¼
    await page.waitForSelector('[data-testid="coaching-panel"]');
    
    // Then: ì½”ì¹­ ì¹´ë“œë“¤ì´ í‘œì‹œë¨
    await expect(page.locator('[data-testid="coaching-card"]')).toHaveCount.greaterThan(0);
    
    // Then: ë¦¬ë” ì •ë³´ê°€ í‘œì‹œë¨
    await expect(page.locator('[data-testid="leader-info"]')).toBeVisible();
  });
  
  test('complete survey submission flow', async ({ page }) => {
    // Given: ì„¤ë¬¸ í˜ì´ì§€ ì ‘ì†
    await page.goto('/survey/new');
    
    // When: ì„¤ë¬¸ ì‘ë‹µ ì…ë ¥ (31ë¬¸í•­)
    for (let i = 1; i <= 31; i++) {
      const question = page.locator(`[data-testid="question-${i}"]`);
      const rating = Math.floor(Math.random() * 7) + 1; // 1-7 ëœë¤
      await question.locator(`input[value="${rating}"]`).check();
    }
    
    // When: ì„¤ë¬¸ ì œì¶œ
    await page.click('[data-testid="submit-survey"]');
    
    // Then: ì„±ê³µ ë©”ì‹œì§€ í‘œì‹œ
    await expect(page.locator('text=ì„¤ë¬¸ì´ ì„±ê³µì ìœ¼ë¡œ ì œì¶œë˜ì—ˆìŠµë‹ˆë‹¤')).toBeVisible();
    
    // Then: ëŒ€ì‹œë³´ë“œë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
    await page.waitForURL('/dashboard');
    
    // Then: ìƒˆë¡œìš´ ì ìˆ˜ê°€ 3D ê·¸ë¦¬ë“œì— ë°˜ì˜ë¨
    await page.waitForSelector('[data-testid="leadership-grid-3d"]');
    await expect(page.locator('[data-testid="leader-point"]')).toBeVisible();
  });
  
  test('should handle real-time updates', async ({ page, context }) => {
    // Given: ë‘ ê°œì˜ í˜ì´ì§€ (ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ì‹œë®¬ë ˆì´ì…˜)
    const page1 = page;
    const page2 = await context.newPage();
    
    await page1.goto('/dashboard');
    await page2.goto('/dashboard');
    
    // When: í•œ í˜ì´ì§€ì—ì„œ ì„¤ë¬¸ ì œì¶œ
    await page2.goto('/survey/new');
    
    // ì„¤ë¬¸ ì‘ì„±
    for (let i = 1; i <= 31; i++) {
      await page2.locator(`[data-testid="question-${i}"] input[value="7"]`).check();
    }
    await page2.click('[data-testid="submit-survey"]');
    
    // Then: ë‹¤ë¥¸ í˜ì´ì§€ì˜ ëŒ€ì‹œë³´ë“œê°€ ìë™ ì—…ë°ì´íŠ¸ë¨
    await page1.waitForTimeout(2000); // ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ëŒ€ê¸°
    
    // ìƒˆë¡œìš´ ë°ì´í„°ê°€ ë°˜ì˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
    const leaderPoints = page1.locator('[data-testid="leader-point"]');
    await expect(leaderPoints).toHaveCount.greaterThan(0);
  });
});
```

### 5.2 ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í¬í•¨ E2E

```typescript
// tests/e2e/performance.spec.ts
import { test, expect } from '@playwright/test';

test.describe('Performance E2E Tests', () => {
  test('dashboard should load within 3 seconds', async ({ page }) => {
    const startTime = Date.now();
    
    await page.goto('/dashboard');
    await page.waitForSelector('[data-testid="leadership-grid-3d"]');
    
    const loadTime = Date.now() - startTime;
    expect(loadTime).toBeLessThan(3000);
  });
  
  test('3D rendering should be smooth with 50+ leaders', async ({ page }) => {
    // Given: ë§ì€ ë¦¬ë” ë°ì´í„°ê°€ ìˆëŠ” í˜ì´ì§€
    await page.goto('/dashboard?demo=large-dataset');
    
    // When: 3D ê·¸ë¦¬ë“œ ë¡œë”©
    await page.waitForSelector('[data-testid="leadership-grid-3d"]');
    
    // Then: ëª¨ë“  í¬ì¸íŠ¸ê°€ ë Œë”ë§ë¨
    const leaderPoints = page.locator('[data-testid="leader-point"]');
    await expect(leaderPoints).toHaveCount.greaterThanOrEqual(50);
    
    // Then: í”„ë ˆì„ë¥  ì²´í¬ (ê°„ì ‘ì )
    const performanceMetrics = await page.evaluate(() => {
      return JSON.parse(JSON.stringify(performance.getEntriesByType('navigation')));
    });
    
    expect(performanceMetrics[0].loadEventEnd - performanceMetrics[0].fetchStart).toBeLessThan(5000);
  });
});
```

## 6. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

### 6.1 ë¡œë“œ í…ŒìŠ¤íŠ¸

```python
# tests/performance/load_test.py
import asyncio
import aiohttp
import time
from dataclasses import dataclass
from typing import List

@dataclass
class LoadTestResult:
    total_requests: int
    successful_requests: int
    failed_requests: int
    avg_response_time: float
    max_response_time: float
    min_response_time: float

class LoadTester:
    def __init__(self, base_url: str):
        self.base_url = base_url
        self.response_times: List[float] = []
        self.successful_count = 0
        self.failed_count = 0
    
    async def make_request(self, session: aiohttp.ClientSession, endpoint: str):
        """ë‹¨ì¼ ìš”ì²­ ì‹¤í–‰"""
        start_time = time.time()
        try:
            async with session.get(f"{self.base_url}{endpoint}") as response:
                await response.text()
                response_time = time.time() - start_time
                self.response_times.append(response_time)
                
                if response.status == 200:
                    self.successful_count += 1
                else:
                    self.failed_count += 1
                    
        except Exception as e:
            self.failed_count += 1
            response_time = time.time() - start_time
            self.response_times.append(response_time)
    
    async def run_load_test(self, endpoint: str, concurrent_users: int, requests_per_user: int) -> LoadTestResult:
        """ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        async with aiohttp.ClientSession() as session:
            tasks = []
            
            for user in range(concurrent_users):
                for request in range(requests_per_user):
                    task = self.make_request(session, endpoint)
                    tasks.append(task)
            
            await asyncio.gather(*tasks)
        
        total_requests = len(self.response_times)
        avg_response_time = sum(self.response_times) / len(self.response_times)
        
        return LoadTestResult(
            total_requests=total_requests,
            successful_requests=self.successful_count,
            failed_requests=self.failed_count,
            avg_response_time=avg_response_time,
            max_response_time=max(self.response_times),
            min_response_time=min(self.response_times)
        )

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
async def test_api_performance():
    """API ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    tester = LoadTester("http://localhost:8000")
    
    # 200 ë™ì‹œ ì‚¬ìš©ì, ê°ê° 5ê°œ ìš”ì²­
    result = await tester.run_load_test("/api/v1/leaders", 200, 5)
    
    # ì„±ëŠ¥ ê¸°ì¤€ ê²€ì¦
    assert result.avg_response_time < 1.0  # í‰ê·  ì‘ë‹µì‹œê°„ 1ì´ˆ ì´í•˜
    assert result.max_response_time < 5.0  # ìµœëŒ€ ì‘ë‹µì‹œê°„ 5ì´ˆ ì´í•˜
    assert result.successful_requests / result.total_requests > 0.99  # 99% ì„±ê³µë¥ 

if __name__ == "__main__":
    asyncio.run(test_api_performance())
```

### 6.2 ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

```python
# tests/performance/db_performance_test.py
import pytest
import asyncio
import time
from sqlalchemy.ext.asyncio import AsyncSession
from app.repositories.leader_repository import LeaderRepository

class TestDatabasePerformance:
    @pytest.mark.asyncio
    async def test_large_dataset_query_performance(self, db_session: AsyncSession):
        """ëŒ€ìš©ëŸ‰ ë°ì´í„° ì¿¼ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        repo = LeaderRepository(db_session)
        
        # Given: 1000ëª…ì˜ ë¦¬ë” ë°ì´í„° ìƒì„± (í…ŒìŠ¤íŠ¸ ì „ì— ë¯¸ë¦¬ ì¤€ë¹„)
        start_time = time.time()
        
        # When: ì „ì²´ ë¦¬ë” ì¡°íšŒ
        leaders = await repo.get_all_with_scores()
        
        # Then: ì¿¼ë¦¬ ì‹œê°„ì´ 2ì´ˆ ì´í•˜
        query_time = time.time() - start_time
        assert query_time < 2.0
        
        # Then: ë°ì´í„° ë¬´ê²°ì„± í™•ì¸
        assert len(leaders) >= 1000
        for leader in leaders[:10]:  # ìƒ˜í”Œ í™•ì¸
            assert leader.current_scores is not None
    
    @pytest.mark.asyncio
    async def test_concurrent_write_operations(self, db_session: AsyncSession):
        """ë™ì‹œ ì“°ê¸° ì‘ì—… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        repo = LeaderRepository(db_session)
        
        async def create_survey_response(leader_id: str, answers: List[int]):
            """ì„¤ë¬¸ ì‘ë‹µ ìƒì„±"""
            return await repo.create_survey_response({
                "leader_id": leader_id,
                "answers": answers
            })
        
        # Given: ê¸°ì¡´ ë¦¬ë” IDë“¤
        leaders = await repo.get_all()
        leader_ids = [leader.id for leader in leaders[:50]]
        
        # When: 50ê°œì˜ ë™ì‹œ ì„¤ë¬¸ ì‘ë‹µ ìƒì„±
        start_time = time.time()
        
        tasks = [
            create_survey_response(leader_id, [5] * 31)
            for leader_id in leader_ids
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Then: ì²˜ë¦¬ ì‹œê°„ì´ 10ì´ˆ ì´í•˜
        processing_time = time.time() - start_time
        assert processing_time < 10.0
        
        # Then: ëª¨ë“  ì‘ì—… ì„±ê³µ
        successful_results = [r for r in results if not isinstance(r, Exception)]
        assert len(successful_results) == 50
```

## 7. í…ŒìŠ¤íŠ¸ ë°ì´í„° ê´€ë¦¬

### 7.1 í…ŒìŠ¤íŠ¸ í”½ìŠ¤ì²˜

```python
# tests/conftest.py
import pytest
import asyncio
from typing import AsyncGenerator
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
from sqlalchemy.orm import sessionmaker
from app.core.database import Base
from app.models.leader import Leader
from app.models.survey_response import SurveyResponse

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ë² ì´ìŠ¤ URL
TEST_DATABASE_URL = "postgresql+asyncpg://test:test@localhost:5432/grid3_test"

@pytest.fixture(scope="session")
def event_loop():
    """ì„¸ì…˜ ë ˆë²¨ ì´ë²¤íŠ¸ ë£¨í”„"""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()

@pytest.fixture(scope="session")
async def test_engine():
    """í…ŒìŠ¤íŠ¸ ë°ì´í„°ë² ì´ìŠ¤ ì—”ì§„"""
    engine = create_async_engine(TEST_DATABASE_URL, echo=False)
    
    # í…Œì´ë¸” ìƒì„±
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)
    
    yield engine
    
    # í…ŒìŠ¤íŠ¸ í›„ ì •ë¦¬
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.drop_all)
    
    await engine.dispose()

@pytest.fixture
async def db_session(test_engine) -> AsyncGenerator[AsyncSession, None]:
    """í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜"""
    async_session = sessionmaker(
        test_engine, class_=AsyncSession, expire_on_commit=False
    )
    
    async with async_session() as session:
        yield session
        await session.rollback()

@pytest.fixture
async def sample_leader(db_session: AsyncSession) -> Leader:
    """ìƒ˜í”Œ ë¦¬ë” ë°ì´í„°"""
    leader = Leader(
        name="í…ŒìŠ¤íŠ¸ë¦¬ë”",
        email="test@example.com",
        team="engineering",
        position="Senior Developer"
    )
    
    db_session.add(leader)
    await db_session.commit()
    await db_session.refresh(leader)
    
    return leader

@pytest.fixture
async def sample_survey_response(db_session: AsyncSession, sample_leader: Leader) -> SurveyResponse:
    """ìƒ˜í”Œ ì„¤ë¬¸ ì‘ë‹µ ë°ì´í„°"""
    response = SurveyResponse(
        leader_id=sample_leader.id,
        answers=[6] * 31,  # ì¤‘ë„í˜• íŒ¨í„´
        people_concern=6.0,
        production_concern=6.0,
        candor_level=6.0,
        lmx_quality=6.0,
        leadership_style="ì¤‘ë„í˜•(6.0, 6.0)"
    )
    
    db_session.add(response)
    await db_session.commit()
    await db_session.refresh(response)
    
    return response
```

### 7.2 í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒ©í† ë¦¬

```python
# tests/factories.py
import factory
from factory import Faker, LazyAttribute
from app.models.leader import Leader
from app.models.survey_response import SurveyResponse

class LeaderFactory(factory.Factory):
    class Meta:
        model = Leader
    
    name = Faker('name', locale='ko_KR')
    email = Faker('email')
    team = Faker('random_element', elements=['engineering', 'product', 'design', 'marketing'])
    position = Faker('job')

class SurveyResponseFactory(factory.Factory):
    class Meta:
        model = SurveyResponse
    
    # ë¦¬ë”ì‹­ ìŠ¤íƒ€ì¼ë³„ ì‘ë‹µ íŒ¨í„´
    answers = LazyAttribute(lambda obj: obj._generate_answers_for_style(obj.style))
    
    @staticmethod
    def _generate_answers_for_style(style: str):
        """ë¦¬ë”ì‹­ ìŠ¤íƒ€ì¼ì— ë”°ë¥¸ ì‘ë‹µ íŒ¨í„´ ìƒì„±"""
        if style == 'team':
            # íŒ€í˜•: ë†’ì€ ì‚¬ëŒ/ì„±ê³¼ ê´€ì‹¬
            return [7, 8] * 8 + [7] * 4 + [6] * 4 + [7] * 7
        elif style == 'task':
            # ê³¼ì—…í˜•: ë‚®ì€ ì‚¬ëŒ, ë†’ì€ ì„±ê³¼
            return [3, 2] * 4 + [8, 9] * 4 + [5] * 8 + [6] * 7
        else:
            # ê¸°ë³¸ê°’: ì¤‘ë„í˜•
            return [5, 6] * 15 + [5]

# ì‚¬ìš© ì˜ˆì‹œ
def test_with_factory_data():
    team_leader = LeaderFactory(team='engineering')
    task_response = SurveyResponseFactory(style='task', leader_id=team_leader.id)
```

## 8. CI/CD í…ŒìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸

### 8.1 GitHub Actions ì›Œí¬í”Œë¡œìš°

```yaml
# .github/workflows/test.yml
name: Test Suite

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]

env:
  NODE_VERSION: '20'
  PYTHON_VERSION: '3.12'

jobs:
  frontend-tests:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      - name: Install dependencies
        run: npm ci
      
      - name: Run TypeScript check
        run: npm run type-check
      
      - name: Run ESLint
        run: npm run lint
      
      - name: Run unit tests
        run: npm run test:ci
      
      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage/lcov.info
          flags: frontend

  backend-tests:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: grid3_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-test.txt
      
      - name: Run backend tests
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/grid3_test
        run: |
          pytest tests/ -v --cov=app --cov-report=xml
      
      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: backend

  e2e-tests:
    runs-on: ubuntu-latest
    needs: [frontend-tests, backend-tests]
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      - name: Install dependencies
        run: npm ci
      
      - name: Install Playwright
        run: npx playwright install chromium
      
      - name: Build application
        run: npm run build
      
      - name: Start application
        run: |
          npm run start &
          sleep 10
      
      - name: Run E2E tests
        run: npm run test:e2e
      
      - name: Upload E2E artifacts
        if: failure()
        uses: actions/upload-artifact@v3
        with:
          name: playwright-report
          path: playwright-report/

  performance-tests:
    runs-on: ubuntu-latest
    needs: [frontend-tests, backend-tests]
    if: github.ref == 'refs/heads/main'
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Run load tests
        run: |
          pip install aiohttp pytest-asyncio
          python tests/performance/load_test.py
      
      - name: Upload performance results
        uses: actions/upload-artifact@v3
        with:
          name: performance-results
          path: performance-results.json
```

### 8.2 í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# scripts/run-all-tests.sh

set -e

echo "ğŸš€ Grid 3.0 ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export NODE_ENV=test
export DATABASE_URL=postgresql://test:test@localhost:5432/grid3_test

# 1. í”„ë¡ íŠ¸ì—”ë“œ í…ŒìŠ¤íŠ¸
echo "ğŸ“± í”„ë¡ íŠ¸ì—”ë“œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰..."
npm run test:ci

# 2. ë°±ì—”ë“œ í…ŒìŠ¤íŠ¸
echo "ğŸ–¥ï¸  ë°±ì—”ë“œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰..."
pytest tests/ -v --cov=app

# 3. í†µí•© í…ŒìŠ¤íŠ¸
echo "ğŸ”— í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰..."
pytest tests/integration/ -v

# 4. E2E í…ŒìŠ¤íŠ¸
echo "ğŸŒ E2E í…ŒìŠ¤íŠ¸ ì‹¤í–‰..."
npm run test:e2e

# 5. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (ì„ íƒì )
if [[ "$1" == "--performance" ]]; then
    echo "âš¡ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰..."
    python tests/performance/load_test.py
fi

echo "âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!"
```

## 9. í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­

### 9.1 ì»¤ë²„ë¦¬ì§€ ëª©í‘œ

```yaml
coverage_targets:
  frontend:
    statements: 85%
    branches: 80%
    functions: 85%
    lines: 85%
    
  backend:
    statements: 90%
    branches: 85%
    functions: 90%
    lines: 90%
    
  critical_modules:
    leadership_calculator: 95%
    api_endpoints: 90%
    database_models: 95%
```

### 9.2 ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

```yaml
performance_benchmarks:
  api_response_time:
    p50: "< 200ms"
    p95: "< 1000ms"
    p99: "< 2000ms"
    
  database_queries:
    simple_select: "< 50ms"
    complex_join: "< 500ms"
    bulk_insert: "< 2000ms"
    
  frontend_rendering:
    initial_load: "< 3000ms"
    3d_grid_render: "< 1000ms"
    component_update: "< 100ms"
```

### 9.3 í…ŒìŠ¤íŠ¸ í’ˆì§ˆ ë©”íŠ¸ë¦­

```typescript
// scripts/test-metrics.ts
interface TestMetrics {
  totalTests: number;
  passedTests: number;
  failedTests: number;
  skippedTests: number;
  averageExecutionTime: number;
  coveragePercentage: number;
  flakyTestCount: number;
}

export async function generateTestReport(): Promise<TestMetrics> {
  const jestResults = await runJestTests();
  const playwrightResults = await runPlaywrightTests();
  
  return {
    totalTests: jestResults.numTotalTests + playwrightResults.totalTests,
    passedTests: jestResults.numPassedTests + playwrightResults.passedTests,
    failedTests: jestResults.numFailedTests + playwrightResults.failedTests,
    skippedTests: jestResults.numPendingTests,
    averageExecutionTime: calculateAverageTime([jestResults, playwrightResults]),
    coveragePercentage: jestResults.coverageMap.getCoverageSummary().statements.pct,
    flakyTestCount: detectFlakyTests()
  };
}
```

## 10. í…ŒìŠ¤íŠ¸ ë„êµ¬ ë° ì„¤ì •

### 10.1 Jest ì„¤ì •

```javascript
// jest.config.js
module.exports = {
  preset: 'ts-jest',
  testEnvironment: 'jsdom',
  setupFilesAfterEnv: ['<rootDir>/tests/setup.ts'],
  moduleNameMapping: {
    '^@/(.*)$': '<rootDir>/src/$1',
    '\\.(css|less|scss|sass)$': 'identity-obj-proxy'
  },
  collectCoverageFrom: [
    'src/**/*.{ts,tsx}',
    '!src/**/*.d.ts',
    '!src/**/*.stories.tsx',
    '!src/main.tsx'
  ],
  coverageThreshold: {
    global: {
      branches: 80,
      functions: 85,
      lines: 85,
      statements: 85
    },
    'src/lib/': {
      branches: 90,
      functions: 95,
      lines: 95,
      statements: 95
    }
  },
  testTimeout: 10000
};
```

### 10.2 Playwright ì„¤ì •

```typescript
// playwright.config.ts
import { defineConfig, devices } from '@playwright/test';

export default defineConfig({
  testDir: './tests/e2e',
  fullyParallel: true,
  forbidOnly: !!process.env.CI,
  retries: process.env.CI ? 2 : 0,
  workers: process.env.CI ? 1 : undefined,
  reporter: [
    ['html'],
    ['junit', { outputFile: 'test-results/junit.xml' }]
  ],
  use: {
    baseURL: 'http://localhost:3000',
    trace: 'on-first-retry',
    screenshot: 'only-on-failure'
  },
  projects: [
    {
      name: 'chromium',
      use: { ...devices['Desktop Chrome'] }
    },
    {
      name: 'firefox', 
      use: { ...devices['Desktop Firefox'] }
    },
    {
      name: 'webkit',
      use: { ...devices['Desktop Safari'] }
    }
  ],
  webServer: {
    command: 'npm run dev',
    port: 3000,
    reuseExistingServer: !process.env.CI
  }
});
```

### 10.3 Pytest ì„¤ì •

```ini
# pytest.ini
[tool:pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = 
    -v
    --tb=short
    --cov=app
    --cov-report=term-missing
    --cov-report=html:htmlcov
    --cov-report=xml
    --cov-fail-under=85
asyncio_mode = auto
markers =
    slow: marks tests as slow (deselect with '-m "not slow"')
    integration: marks tests as integration tests
    e2e: marks tests as end-to-end tests
    performance: marks tests as performance tests
```

---

ì´ í…ŒìŠ¤íŠ¸ ì „ëµì„ ë”°ë¼ Grid 3.0 ë¦¬ë”ì‹­ ë§¤í•‘ í”Œë«í¼ì˜ í’ˆì§ˆê³¼ ì•ˆì •ì„±ì„ ë³´ì¥í•˜ê² ì†Œ, TJë‹˜! ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ìë™í™”ë˜ì–´ ê°œë°œ ê³¼ì •ì—ì„œ ì§€ì†ì ìœ¼ë¡œ ê²€ì¦ë  ê²ƒì´ì˜¤.